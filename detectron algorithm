#all the contents in order 
from google.colab import drive
drive.mount('/content/drive')


import torch
print(torch.cuda.is_available())  # Should print True
print(torch.cuda.get_device_name(0))  # Should print "NVIDIA GeForce RTX 2050"



import os
dataset_path = '/content/drive/My Drive/ceew project/train dataset'
os.listdir(dataset_path)


# Install Detectron2 for PyTorch 2.5.1 and CUDA 12.1
!pip install 'git+https://github.com/facebookresearch/detectron2.git@main'



from PIL import Image
import json
import os

def vgg_to_coco(vgg_json_path, output_path, image_dir):
    with open(vgg_json_path) as f:
        vgg_data = json.load(f)

    coco_format = {
        "images": [],
        "annotations": [],
        "categories": [{"id": 1, "name": "waste"}, {"id": 2, "name": "not waste"}]
    }

    annotation_id = 1
    for image_id, (filename, file_data) in enumerate(vgg_data.items()):
        img_path = os.path.join(image_dir, filename)

        # Load image to get dimensions
        try:
            with Image.open(img_path) as img:
                width, height = img.size
        except FileNotFoundError:
            print(f"Image {filename} not found in {image_dir}. Skipping.")
            continue

        # Add image metadata
        coco_format['images'].append({
            "id": image_id,
            "file_name": filename,
            "width": width,
            "height": height
        })

        # Process regions (polygons)
        for region_id, region in file_data['regions'].items():
            shape_attributes = region['shape_attributes']
            region_attributes = region['region_attributes']

            if 'all_points_x' not in shape_attributes or 'all_points_y' not in shape_attributes:
                print(f"Skipping region {region_id} in {filename} due to missing points.")
                continue

            x_points = shape_attributes['all_points_x']
            y_points = shape_attributes['all_points_y']
            polygon = [[x, y] for x, y in zip(x_points, y_points)]

            label = region_attributes.get('label', '').strip().lower()
            category_id = 1 if label == 'waste' else 2 if label == 'not waste' else None

            if category_id is None:
                print(f"Skipping region {region_id} in {filename} due to unknown label: {label}")
                continue

            coco_format['annotations'].append({
                "id": annotation_id,
                "image_id": image_id,
                "category_id": category_id,
                "segmentation": [sum(polygon, [])],  # Flatten the polygon list
                "area": 0,  # Optional: Compute polygon area if needed
                "bbox": [
                    min(x_points),
                    min(y_points),
                    max(x_points) - min(x_points),
                    max(y_points) - min(y_points)
                ],
                "iscrowd": 0
            })
            annotation_id += 1

    # Save COCO annotations
    with open(output_path, 'w') as f:
        json.dump(coco_format, f, indent=4)






vgg_json_path = '/content/drive/MyDrive/ceew project/vgg json /labels_my-project-name_2025-01-16-04-53-24.json'
output_coco_path = '/content/drive/MyDrive/ceew project/vgg json /coco_annotations.json'  # Path to save converted COCO file
image_dir = '/content/drive/MyDrive/ceew project/train dataset'  # Update if images are in a different directory

# Convert VGG JSON to COCO format
vgg_to_coco(vgg_json_path, output_coco_path, image_dir)







import json

with open('/content/drive/MyDrive/ceew project/vgg json /labels_my-project-name_2025-01-16-04-53-24.json', 'r') as f:
    vgg_data = json.load(f)

print(vgg_data)








import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import DatasetCatalog, MetadataCatalog
from detectron2.data.datasets import register_coco_instances

# Register the dataset
register_coco_instances("waste_segmentation", {}, output_coco_path, image_dir)

# Verify dataset
from detectron2.data import DatasetCatalog
dataset_dicts = DatasetCatalog.get("waste_segmentation")
print(f"Number of images: {len(dataset_dicts)}")







import cv2
from google.colab.patches import cv2_imshow
from detectron2.utils.visualizer import Visualizer

for d in dataset_dicts[:5]:
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get("waste_segmentation"))
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])  # Use cv2_imshow instead of cv2.imshow




from detectron2.config import get_cfg
from detectron2.engine import DefaultTrainer
from detectron2.model_zoo import get_config_file, get_checkpoint_url

cfg = get_cfg()
cfg.merge_from_file(get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("waste_segmentation",)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")  # Pretrained weights
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 300  # Adjust based on dataset size
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only 'waste'

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()






from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

evaluator = COCOEvaluator("waste_segmentation", cfg, False, output_dir="./output/")
val_loader = build_detection_test_loader(cfg, "waste_segmentation")
print(inference_on_dataset(trainer.model, val_loader, evaluator))








import random
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

metadata = MetadataCatalog.get("waste_segmentation")
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    outputs = predictor(img)
    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.8)
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2.imshow(v.get_image()[:, :, ::-1])






from detectron2.engine import DefaultPredictor

cfg.MODEL.WEIGHTS = "/content/output/model_final.pth"  # Path to trained weights
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a testing confidence threshold
cfg.MODEL.DEVICE = "cpu"  # Use "cuda" if GPU is available
predictor = DefaultPredictor(cfg)






import random
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
from google.colab.patches import cv2_imshow  # For displaying images in Colab

metadata = MetadataCatalog.get("waste_segmentation")
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    outputs = predictor(img)
    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.8)
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(v.get_image()[:, :, ::-1])  # Use cv2_imshow for Colab






import cv2
from google.colab.patches import cv2_imshow
from detectron2.utils.visualizer import Visualizer

# Path to the image
image_path = /content/image_253.jpg

# Load image
img = cv2.imread(image_path)

# Run inference
outputs = predictor(img)

# Visualize the results
v = Visualizer(img[:, :, ::-1], metadata=None, scale=0.8)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2_imshow(v.get_image()[:, :, ::-1])  # Display the image in Colab




torch.save(trainer.model.state_dict(), "waste_segmentation_model.pth")
